<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-06-08T06:26:35.404Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://example.com/wiki/Webpack/"/>
    <id>http://example.com/wiki/Webpack/</id>
    <published>2025-06-08T06:26:35.404Z</published>
    <updated>2025-06-08T06:26:35.404Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Spark HA &amp; Yarn配置</title>
    <link href="http://example.com/wiki/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/wiki/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/</id>
    <published>2025-06-08T06:26:19.000Z</published>
    <updated>2025-06-08T08:22:28.699Z</updated>
    
    <content type="html"><![CDATA[<h1 id="配置Spark-HA"><a href="#配置Spark-HA" class="headerlink" title="配置Spark HA"></a>配置Spark HA</h1><h2 id="1-停止spark-集群"><a href="#1-停止spark-集群" class="headerlink" title="1.停止spark 集群"></a>1.停止spark 集群</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$SPARK_HOME</span>/sbin/stop-all.sh</span><br></pre></td></tr></table></figure><h2 id="2-修改改配置"><a href="#2-修改改配置" class="headerlink" title="2.修改改配置"></a>2.修改改配置</h2><h3 id="修改spark-env-sh文件"><a href="#修改spark-env-sh文件" class="headerlink" title="修改spark-env.sh文件"></a>修改spark-env.sh文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//进入 spark 配置文件目录</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span>/conf</span><br><span class="line">//打开 spark-env.sh文件</span><br><span class="line">vim spark-env.sh</span><br><span class="line">//注释掉 Master配置</span><br><span class="line"><span class="comment"># export SPARK_MASTER_HOST=hadoop01</span></span><br><span class="line">//添加SPARK_DAEMON_JAVA_OPTS,内容如下:</span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER  -Dspark.deploy.zookeeper.url=hadoop01:2181,hadoop02:2181,hadoop03:2181  -Dspark.deploy.zookeeper.dir=/spark&quot;</span></span><br><span class="line">//保存退出</span><br><span class="line">:wq</span><br></pre></td></tr></table></figure><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.deploy.recoveryMode：恢复模式</span><br><span class="line">spark.deploy.zookeeper.url：ZooKeeper的Server地址</span><br><span class="line">spark.deploy.zookeeper.dir：保存集群元数据信息的文件、目录。包括Worker、Driver、Application信息。</span><br></pre></td></tr></table></figure><h3 id="分发到其它节点"><a href="#分发到其它节点" class="headerlink" title="分发到其它节点"></a>分发到其它节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh hadoop02:<span class="variable">$PWD</span></span><br><span class="line">scp spark-env.sh hadoop03:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3.测试"></a>3.测试</h2><h3 id="启动zk集群博主直接用的脚本"><a href="#启动zk集群博主直接用的脚本" class="headerlink" title="启动zk集群博主直接用的脚本"></a>启动zk集群博主直接用的脚本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zkstart-all.sh</span><br><span class="line">没有的话可以使用 zkServer.sh start 命令启动</span><br><span class="line">没有脚本可以配置参考链接:</span><br><span class="line">https://blog.csdn.net/hongchenshijie/category_9453806.html</span><br></pre></td></tr></table></figure><h3 id="启动spark-集群"><a href="#启动spark-集群" class="headerlink" title="启动spark 集群"></a>启动spark 集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 启动spark (节点一运行)</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/sbin/start-all.sh</span><br><span class="line">//在节点二单独只启动个master</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/sbin/start-master.sh</span><br></pre></td></tr></table></figure><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>1.在普通模式下启动spark集群<br>只需要在主节点上执行<code>start-all.sh </code>就可以了</p><p>2.在高可用模式下启动spark集群<br>先需要在任意一台主节点上执行<code>start-all.sh</code><br>然后在另外一台主节点上单独执行<code>start-master.sh</code></p><h2 id="4-查看hadoop01和hadoop02"><a href="#4-查看hadoop01和hadoop02" class="headerlink" title="4.查看hadoop01和hadoop02"></a>4.查看hadoop01和hadoop02</h2><p>进入web,界面查看状态可以观察到有一台状态为<code>StandBy</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://hadoop01:8080/</span><br><span class="line">http://hadoop02:8080/</span><br></pre></td></tr></table></figure><h2 id="5-先使用jps查看-master-进程-id"><a href="#5-先使用jps查看-master-进程-id" class="headerlink" title="5.先使用jps查看 master 进程 id"></a>5.先使用jps查看 master 进程 id</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><h2 id="6-使用kill-9-杀死master进程"><a href="#6-使用kill-9-杀死master进程" class="headerlink" title="6.使用kill -9 杀死master进程"></a>6.使用kill -9 杀死master进程</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9</span><br></pre></td></tr></table></figure><h2 id="7-然后重新启动节点一的master"><a href="#7-然后重新启动节点一的master" class="headerlink" title="7.然后重新启动节点一的master"></a>7.然后重新启动节点一的master</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 4781</span><br></pre></td></tr></table></figure><h2 id="8-刷新后再次查看web界面"><a href="#8-刷新后再次查看web界面" class="headerlink" title="8.刷新后再次查看web界面"></a>8.刷新后再次查看web界面</h2><p>可以看到节点一的状态编程 standBy 了,节点二的状态变成了alive,这就说明配置成功了</p><h1 id="on-yarn模式"><a href="#on-yarn模式" class="headerlink" title="on yarn模式"></a>on yarn模式</h1><p><code>安装需要启动Hadoop</code><br><code>需要安装单机版Spark</code></p><h2 id="1-修改配置"><a href="#1-修改配置" class="headerlink" title="1.修改配置"></a>1.修改配置</h2><p>在spark-env.sh ，添加HADOOP_CONF_DIR配置，指明了hadoop的配置文件的位置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//打开配置文件</span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$SPARK_HOME</span>/conf</span><br><span class="line">vim spark-env.sh</span><br><span class="line">//添加如下配置</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$&#123;HADOOP_HOME&#125;</span>/etc/hadoop</span><br><span class="line">//分发到其它节点(可以不分发)</span><br><span class="line">scp spark-env.sh hadoop02:<span class="variable">$PWD</span></span><br><span class="line">scp spark-env.sh hadoop03:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><h2 id="2-提交任务到yarn"><a href="#2-提交任务到yarn" class="headerlink" title="2.提交任务到yarn"></a>2.提交任务到yarn</h2><p>先进入 <a href="http://hadoop01:8088/">http://hadoop01:8088</a> yarn页面<br>提交一下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$SPARK_HOME</span>/bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 2 \</span><br><span class="line">--queue default \</span><br><span class="line"><span class="variable">$SPARK_HOME</span>/examples/jars/spark-examples_2.11-2.2.0.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure><p>刷新后看到任务就说明配置成功了</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><blockquote><ul><li><a href="https://blog.csdn.net/hongchenshijie/article/details/105262938?ops_request_misc=%257B%2522request%255Fid%2522%253A%25225813f782ac86ab814991090989c3fd08%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=5813f782ac86ab814991090989c3fd08&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-105262938-null-null.142%5Ev102%5Epc_search_result_base7&utm_term=Spark%20HA%20&%20Yarn%E9%85%8D%E7%BD%AE&spm=1018.2226.3001.4187">Spark集群配置 和 Spark HA 集群配置 以及 Spark on yarn 模式</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;配置Spark-HA&quot;&gt;&lt;a href=&quot;#配置Spark-HA&quot; class=&quot;headerlink&quot; title=&quot;配置Spark HA&quot;&gt;&lt;/a&gt;配置Spark HA&lt;/h1&gt;&lt;h2 id=&quot;1-停止spark-集群&quot;&gt;&lt;a href=&quot;#1-停止spark</summary>
      
    
    
    
    <category term="实训" scheme="http://example.com/categories/%E5%AE%9E%E8%AE%AD/"/>
    
    
    <category term="Spark HA &amp; Yarn" scheme="http://example.com/tags/Spark-HA-Yarn/"/>
    
  </entry>
  
  <entry>
    <title>Spark local&amp; stand-alone配置</title>
    <link href="http://example.com/wiki/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/wiki/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/</id>
    <published>2025-06-08T06:25:46.000Z</published>
    <updated>2025-06-08T08:22:16.938Z</updated>
    
    <content type="html"><![CDATA[<p>Standalone模式：这是Spark自带的简单集群管理器，适用于快速搭建测试集群或小型生产环境。在Standalone模式下，Spark自身作为独立的集群运行，通过启动Spark Master和Spark Worker进程来管理资源和调度作业。</p><h1 id="Spark安装与启动"><a href="#Spark安装与启动" class="headerlink" title="Spark安装与启动"></a>Spark安装与启动</h1><h2 id="1-创建安装目录-所有节点"><a href="#1-创建安装目录-所有节点" class="headerlink" title="1.创建安装目录(所有节点)"></a>1.创建安装目录(所有节点)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/spark</span><br></pre></td></tr></table></figure><h2 id="2-上传安装介质-Master节点-：spark-3-5-3-bin-hadoop3-tgz到安装目录-opt-spark"><a href="#2-上传安装介质-Master节点-：spark-3-5-3-bin-hadoop3-tgz到安装目录-opt-spark" class="headerlink" title="2.上传安装介质(Master节点)：spark-3.5.3-bin-hadoop3.tgz到安装目录&#x2F;opt&#x2F;spark"></a>2.上传安装介质(Master节点)：spark-3.5.3-bin-hadoop3.tgz到安装目录&#x2F;opt&#x2F;spark</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rz -b</span><br></pre></td></tr></table></figure><h2 id="3-解压安装介质"><a href="#3-解压安装介质" class="headerlink" title="3.解压安装介质"></a>3.解压安装介质</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.5.3-bin-hadoop3.tgz</span><br></pre></td></tr></table></figure><h2 id="cd-SPARK-HOME-conf"><a href="#cd-SPARK-HOME-conf" class="headerlink" title="cd ${SPARK_HOME}\conf"></a>cd ${SPARK_HOME}\conf</h2><h2 id="vim-spark-default-conf"><a href="#vim-spark-default-conf" class="headerlink" title="vim spark-default.conf"></a>vim spark-default.conf</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span>               hdfs://hadoop231:9000/user/hadoop/spark/event_log</span><br><span class="line">spark.yarn.historyServer.address hadoop231:19888</span><br><span class="line">spark.history.ui.port 9091</span><br><span class="line">spark.executor.extraJavaOptions   -Xss32M</span><br></pre></td></tr></table></figure><h2 id="vim-spark-env-sh"><a href="#vim-spark-env-sh" class="headerlink" title="vim spark-env.sh"></a>vim spark-env.sh</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/java/jdk1.8.0_411</span><br><span class="line">export HADOOP_HOME=/opt/hadoop/hadoop-3.3.6</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export SPARK_DIST_CLASSPATH=$&#123;HADOOP_HOME&#125;/etc/hadoop:$&#123;HADOOP_HOME&#125;/share/hadoop/common/lib/*:$&#123;HADOOP_HOME&#125;/share/hadoop/common/*:$&#123;HADOOP_HOME&#125;/share/hadoop/hdfs:$&#123;HADOOP_HOME&#125;/share/hadoop/hdfs/lib/*:$&#123;HADOOP_HOME&#125;/share/hadoop/hdfs/*:$&#123;HADOOP_HOME&#125;/share/hadoop/mapreduce/*:$&#123;HADOOP_HOME&#125;/share/hadoop/yarn:$&#123;HADOOP_HOME&#125;/share/hadoop/yarn/lib/*:$&#123;HADOOP_HOME&#125;/share/hadoop/yarn/*</span><br><span class="line">export SPARK_MASTER_HOST=hadoop24</span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line">export SPARK_LOCAL_DIRS=/opt/spark/data</span><br></pre></td></tr></table></figure><h2 id="vim-workders"><a href="#vim-workders" class="headerlink" title="vim workders"></a>vim workders</h2><figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop231</span><br></pre></td></tr></table></figure><h2 id="4-添加工作者节点-Master节点-：编辑workers文件（vim-workers），添加工作者节点主机名或IP地址"><a href="#4-添加工作者节点-Master节点-：编辑workers文件（vim-workers），添加工作者节点主机名或IP地址" class="headerlink" title="4.添加工作者节点(Master节点) ：编辑workers文件（vim workers），添加工作者节点主机名或IP地址"></a>4.添加工作者节点(Master节点) ：编辑workers文件（vim workers），添加工作者节点主机名或IP地址</h2><h2 id="5-配置spark环境参数"><a href="#5-配置spark环境参数" class="headerlink" title="5.配置spark环境参数"></a>5.配置spark环境参数</h2><h2 id="6-分发配置好的spark安装目录"><a href="#6-分发配置好的spark安装目录" class="headerlink" title="6.分发配置好的spark安装目录"></a>6.分发配置好的spark安装目录</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp  -r spark-3.5.3 hadoop@hadoop231:/opt/spark/</span><br></pre></td></tr></table></figure><h2 id="7-配置spark系统环境变量（SPARK-HOME）-所有节点"><a href="#7-配置spark系统环境变量（SPARK-HOME）-所有节点" class="headerlink" title="7.配置spark系统环境变量（SPARK_HOME） (所有节点)"></a>7.配置spark系统环境变量（SPARK_HOME） (所有节点)</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br></pre></td></tr></table></figure><h2 id="8-进入启动目录"><a href="#8-进入启动目录" class="headerlink" title="8.进入启动目录"></a>8.进入启动目录</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/spark/spark-3.5.3/sbin</span><br><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><h2 id="9-查看Master节点和Worker节点是否启动"><a href="#9-查看Master节点和Worker节点是否启动" class="headerlink" title="9.查看Master节点和Worker节点是否启动"></a>9.查看Master节点和Worker节点是否启动</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/spark/spark-3.5.3/sbin</span><br><span class="line">jps</span><br></pre></td></tr></table></figure><h2 id="10-Spark程序测试"><a href="#10-Spark程序测试" class="headerlink" title="10.Spark程序测试"></a>10.Spark程序测试</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pyspark</span><br><span class="line">spark-shell</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Standalone模式：这是Spark自带的简单集群管理器，适用于快速搭建测试集群或小型生产环境。在Standalone模式下，Spark自身作为独立的集群运行，通过启动Spark Master和Spark Worker进程来管理资源和调度作业。&lt;/p&gt;
&lt;h1 id=&quot;</summary>
      
    
    
    
    <category term="实训" scheme="http://example.com/categories/%E5%AE%9E%E8%AE%AD/"/>
    
    
    <category term="Spark" scheme="http://example.com/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark基础环境配置</title>
    <link href="http://example.com/wiki/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://example.com/wiki/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</id>
    <published>2025-06-08T06:25:15.000Z</published>
    <updated>2025-06-08T08:22:02.133Z</updated>
    
    <content type="html"><![CDATA[<h1 id="JDK"><a href="#JDK" class="headerlink" title="JDK"></a>JDK</h1><h2 id="JDK1-8安装"><a href="#JDK1-8安装" class="headerlink" title="JDK1.8安装"></a>JDK1.8安装</h2><p>1.双击安装包jdk-8u301-windows-x64.exe</p><p>2.安装过程中选择指定的JDK安装目录，比如：D:&#x2F;java&#x2F;jdk1.8.0_301</p><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><p>1.配置JAVA_HOME环境变量</p><p>2.配置JDK系统PATH路径</p><p>3.配置CLASSPATH环境变量，.:%JAVA_HOME%\lib&#x2F;dt.jar;%JAVA_HOME%\lib\tools.jar; </p><p>4.安装成功后的测试，在windows控制台窗口输入如下命令，如果能够看到jdk版本信息说明安装成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装测试</span></span><br><span class="line">java -version</span><br></pre></td></tr></table></figure><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="添加hadoop用户和组"><a href="#添加hadoop用户和组" class="headerlink" title="添加hadoop用户和组"></a>添加hadoop用户和组</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建用户组</span></span><br><span class="line"><span class="comment"># sudo groupadd hadoop</span></span><br><span class="line"><span class="comment"># 创建新的用户</span></span><br><span class="line"><span class="built_in">sudo</span> adduser hadoop <span class="comment"># 会自动创建组名为hadoop的用户组</span></span><br><span class="line"><span class="comment"># 将现有用户hadoop添加到用户组hadoop（组名 用户名）</span></span><br><span class="line"><span class="comment"># sudo usermod -a -G hadoop hadoop</span></span><br></pre></td></tr></table></figure><h2 id="给新建用户添加管理员权限和sudo权限"><a href="#给新建用户添加管理员权限和sudo权限" class="headerlink" title="给新建用户添加管理员权限和sudo权限"></a>给新建用户添加管理员权限和sudo权限</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 给新建用户添加root权限和sudo权限</span></span><br><span class="line"><span class="built_in">sudo</span> usermod -a -G adm hadoop</span><br><span class="line"><span class="built_in">sudo</span> usermod -a -G <span class="built_in">sudo</span> hadoop</span><br></pre></td></tr></table></figure><h2 id="安装并开启SSH服务"><a href="#安装并开启SSH服务" class="headerlink" title="安装并开启SSH服务"></a>安装并开启SSH服务</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install openssh-server <span class="comment">#安装ssh服务</span></span><br></pre></td></tr></table></figure><h2 id="设置免密登录"><a href="#设置免密登录" class="headerlink" title="设置免密登录"></a>设置免密登录</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、需要在主机上生成公钥和私钥，四次回车</span></span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、将公钥传递给其他虚拟机</span></span><br><span class="line">ssh-copy-id ...</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="关闭防火墙，修改安全机制"><a href="#关闭防火墙，修改安全机制" class="headerlink" title="关闭防火墙，修改安全机制"></a>关闭防火墙，修改安全机制</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#禁用防火墙</span></span><br><span class="line">systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line"></span><br><span class="line"><span class="comment">#修改安全机制</span></span><br><span class="line">vi /etc/selinux/config</span><br><span class="line"></span><br><span class="line"> 修改⾥⾯的 SELINUX=disabled</span><br></pre></td></tr></table></figure><h2 id="配置hadoop集群"><a href="#配置hadoop集群" class="headerlink" title="配置hadoop集群"></a>配置hadoop集群</h2><h3 id="配置java，hadoop的环境变量"><a href="#配置java，hadoop的环境变量" class="headerlink" title="配置java，hadoop的环境变量"></a>配置java，hadoop的环境变量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置主机的环境变量</span></span><br><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/opt/installs/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#拷贝环境变量到另外两台</span></span><br><span class="line">scp -r /etc/profile hadoop@hadoop:/etc/</span><br><span class="line">scp -r /etc/profile hadoop@hadoop:/etc/</span><br><span class="line"></span><br><span class="line"><span class="comment">#在01，02，03上分别刷新</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h3 id="配置分布式集群环境"><a href="#配置分布式集群环境" class="headerlink" title="配置分布式集群环境"></a>配置分布式集群环境</h3><p>路径：&#x2F;opt&#x2F;installs&#x2F;hadoop&#x2F;etc&#x2F;hadoop</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/installs/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure><p>vi core-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 设置namenode节点 --&gt;</span><br><span class="line">    &lt;!-- 注意: hadoop1.x时代默认端⼝9000 hadoop2.x时代默认端⼝8020 hadoop3.x时 代默认端⼝ 9820 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://hadoop:9820&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- hdfs的基础路径，被其他属性所依赖的⼀个基础路径 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/installs/hadoop/tmp&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>vi hdfs-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!--secondarynamenode守护进程的http地址：主机名和端⼝号。参考守护进程布局 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata02:9868&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- namenode守护进程的http地址：主机名和端⼝号。参考守护进程布局 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata01:9870&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>vi hadoop-env.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/installs/jdk</span><br><span class="line"><span class="comment"># Hadoop3中，需要添加如下配置，设置启动集群⻆⾊的⽤户是谁</span></span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure><p>vi workers</p><h3 id="hadoop和环境变量远程拷贝到其他虚拟机"><a href="#hadoop和环境变量远程拷贝到其他虚拟机" class="headerlink" title="hadoop和环境变量远程拷贝到其他虚拟机"></a>hadoop和环境变量远程拷贝到其他虚拟机</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将主机的hadoop 拷贝给集群组员</span></span><br><span class="line">scp -r /opt/installs/hadoop/  hadoop@hadoop:/opt/installs/</span><br><span class="line">...</span><br><span class="line"><span class="comment">#拷贝环境变量</span></span><br><span class="line">scp -r /etc/profile hadoop@hadoop:/etc/</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>在02 和 03 上刷新环境变量<br>source &#x2F;etc&#x2F;profile</p><h3 id="格式化nameNode"><a href="#格式化nameNode" class="headerlink" title="格式化nameNode"></a>格式化nameNode</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure><h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure><p>通过jps查看进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ jps</span><br></pre></td></tr></table></figure><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><h2 id="1-执行如下命令安装Kafka："><a href="#1-执行如下命令安装Kafka：" class="headerlink" title="1.执行如下命令安装Kafka："></a>1.执行如下命令安装Kafka：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/Downloads <span class="comment">#假设安装文件放在这个目录下</span></span><br><span class="line">$ <span class="built_in">sudo</span> tar -zxvf kafka_2.13-3.9.0.tgz -C /opt/kafka</span><br><span class="line">$ <span class="built_in">cd</span> /opt/kafka</span><br><span class="line">$ <span class="built_in">sudo</span> <span class="built_in">chown</span> -R hadoop:hadoop ./kafka</span><br></pre></td></tr></table></figure><h2 id="2-首先需要启动Kafka。请登录Linux系统（本教程统一使用hadoop用户登录），打开一个终端，输入下面命令启动Zookeeper服务："><a href="#2-首先需要启动Kafka。请登录Linux系统（本教程统一使用hadoop用户登录），打开一个终端，输入下面命令启动Zookeeper服务：" class="headerlink" title="2.首先需要启动Kafka。请登录Linux系统（本教程统一使用hadoop用户登录），打开一个终端，输入下面命令启动Zookeeper服务："></a>2.首先需要启动Kafka。请登录Linux系统（本教程统一使用hadoop用户登录），打开一个终端，输入下面命令启动Zookeeper服务：</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span>  /opt/kafka/kafka_2.13-3.9.0</span><br><span class="line">$ ./bin/zookeeper-server-start.sh  config/zookeeper.properties</span><br></pre></td></tr></table></figure><p>注意，执行上面命令以后，终端窗口会返回一堆信息，然后就停住不动了，没有回到Shell命令提示符状态，这时，不要误以为死机了，而是Zookeeper服务器已经启动，正在处于服务状态。所以，不要关闭这个终端窗口，一旦关闭，Zookeeper服务就停止了。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><blockquote><ul><li><a href="https://blog.csdn.net/ningbw2000/article/details/142102415?spm=1001.2014.3001.5502">大数据平台Hadoop实验环境部署（完全分布式集群模式）</a></li><li><a href="https://blog.csdn.net/m0_53042880/article/details/147904132?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522af13f57b7ffb86265690478bde76a1a4%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=af13f57b7ffb86265690478bde76a1a4&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-147904132-null-null.142%5Ev102%5Epc_search_result_base7&utm_term=%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAhadoop%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4&spm=1018.2226.3001.4187">Hadoop集群完全分布式搭建</a></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;JDK&quot;&gt;&lt;a href=&quot;#JDK&quot; class=&quot;headerlink&quot; title=&quot;JDK&quot;&gt;&lt;/a&gt;JDK&lt;/h1&gt;&lt;h2 id=&quot;JDK1-8安装&quot;&gt;&lt;a href=&quot;#JDK1-8安装&quot; class=&quot;headerlink&quot; title=&quot;JDK1.</summary>
      
    
    
    
    <category term="实训" scheme="http://example.com/categories/%E5%AE%9E%E8%AE%AD/"/>
    
    
    <category term="JDK，Hadoop，Zookeeper" scheme="http://example.com/tags/JDK%EF%BC%8CHadoop%EF%BC%8CZookeeper/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/wiki/hello-world/"/>
    <id>http://example.com/wiki/hello-world/</id>
    <published>2025-06-04T05:57:11.603Z</published>
    <updated>2025-06-07T08:19:11.585Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
