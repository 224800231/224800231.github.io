{"pages":[{"title":"Categories","date":"2025-06-04T07:14:56.862Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2025-06-04T07:14:56.862Z","path":"tags/index.html","text":""},{"title":"About","date":"2025-06-04T07:14:56.862Z","path":"about/index.html","text":""}],"posts":[{"title":"","date":"2025-06-08T06:26:35.404Z","path":"wiki/Webpack/","text":"","tags":[],"categories":[]},{"title":"Spark HA & Yarn配置","date":"2025-06-08T06:26:19.000Z","path":"wiki/Spark-HA-Yarn配置/","text":"配置Spark HA1.停止spark 集群1$SPARK_HOME/sbin/stop-all.sh 2.修改改配置修改spark-env.sh文件12345678910//进入 spark 配置文件目录cd $SPARK_HOME/conf//打开 spark-env.sh文件vim spark-env.sh//注释掉 Master配置# export SPARK_MASTER_HOST=hadoop01//添加SPARK_DAEMON_JAVA_OPTS,内容如下:export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop01:2181,hadoop02:2181,hadoop03:2181 -Dspark.deploy.zookeeper.dir=/spark&quot;//保存退出:wq 参数说明123spark.deploy.recoveryMode：恢复模式spark.deploy.zookeeper.url：ZooKeeper的Server地址spark.deploy.zookeeper.dir：保存集群元数据信息的文件、目录。包括Worker、Driver、Application信息。 分发到其它节点12scp spark-env.sh hadoop02:$PWDscp spark-env.sh hadoop03:$PWD 3.测试启动zk集群博主直接用的脚本1234zkstart-all.sh没有的话可以使用 zkServer.sh start 命令启动没有脚本可以配置参考链接:https://blog.csdn.net/hongchenshijie/category_9453806.html 启动spark 集群1234// 启动spark (节点一运行)$SPARK_HOME/sbin/start-all.sh//在节点二单独只启动个master$SPARK_HOME/sbin/start-master.sh 注意1.在普通模式下启动spark集群只需要在主节点上执行start-all.sh 就可以了 2.在高可用模式下启动spark集群先需要在任意一台主节点上执行start-all.sh然后在另外一台主节点上单独执行start-master.sh 4.查看hadoop01和hadoop02进入web,界面查看状态可以观察到有一台状态为StandBy 12http://hadoop01:8080/http://hadoop02:8080/ 5.先使用jps查看 master 进程 id1jps 6.使用kill -9 杀死master进程1kill -9 7.然后重新启动节点一的master1kill -9 4781 8.刷新后再次查看web界面可以看到节点一的状态编程 standBy 了,节点二的状态变成了alive,这就说明配置成功了 on yarn模式安装需要启动Hadoop需要安装单机版Spark 1.修改配置在spark-env.sh ，添加HADOOP_CONF_DIR配置，指明了hadoop的配置文件的位置 12345678//打开配置文件cd $SPARK_HOME/confvim spark-env.sh//添加如下配置export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop//分发到其它节点(可以不分发)scp spark-env.sh hadoop02:$PWDscp spark-env.sh hadoop03:$PWD 2.提交任务到yarn先进入 http://hadoop01:8088 yarn页面提交一下命令 12345678910$SPARK_HOME/bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode client \\--driver-memory 1g \\--executor-memory 1g \\--executor-cores 2 \\--queue default \\$SPARK_HOME/examples/jars/spark-examples_2.11-2.2.0.jar \\10 刷新后看到任务就说明配置成功了 参考资料 Spark集群配置 和 Spark HA 集群配置 以及 Spark on yarn 模式","tags":[{"name":"Spark HA & Yarn","slug":"Spark-HA-Yarn","permalink":"http://example.com/tags/Spark-HA-Yarn/"}],"categories":[{"name":"实训","slug":"实训","permalink":"http://example.com/categories/%E5%AE%9E%E8%AE%AD/"}]},{"title":"Spark local& stand-alone配置","date":"2025-06-08T06:25:46.000Z","path":"wiki/Spark-local-stand-alone配置/","text":"Standalone模式：这是Spark自带的简单集群管理器，适用于快速搭建测试集群或小型生产环境。在Standalone模式下，Spark自身作为独立的集群运行，通过启动Spark Master和Spark Worker进程来管理资源和调度作业。 Spark安装与启动1.创建安装目录(所有节点)1/opt/spark 2.上传安装介质(Master节点)：spark-3.5.3-bin-hadoop3.tgz到安装目录&#x2F;opt&#x2F;spark1rz -b 3.解压安装介质1tar -zxvf spark-3.5.3-bin-hadoop3.tgz cd ${SPARK_HOME}\\confvim spark-default.conf12345spark.eventLog.enabled truespark.eventLog.dir hdfs://hadoop231:9000/user/hadoop/spark/event_logspark.yarn.historyServer.address hadoop231:19888spark.history.ui.port 9091spark.executor.extraJavaOptions -Xss32M vim spark-env.sh12345678export JAVA_HOME=/opt/java/jdk1.8.0_411export HADOOP_HOME=/opt/hadoop/hadoop-3.3.6export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport SPARK_DIST_CLASSPATH=$&#123;HADOOP_HOME&#125;/etc/hadoop:$&#123;HADOOP_HOME&#125;/share/hadoop/common/lib/*:$&#123;HADOOP_HOME&#125;/share/hadoop/common/*:$&#123;HADOOP_HOME&#125;/share/hadoop/hdfs:$&#123;HADOOP_HOME&#125;/share/hadoop/hdfs/lib/*:$&#123;HADOOP_HOME&#125;/share/hadoop/hdfs/*:$&#123;HADOOP_HOME&#125;/share/hadoop/mapreduce/*:$&#123;HADOOP_HOME&#125;/share/hadoop/yarn:$&#123;HADOOP_HOME&#125;/share/hadoop/yarn/lib/*:$&#123;HADOOP_HOME&#125;/share/hadoop/yarn/*export SPARK_MASTER_HOST=hadoop24export SPARK_MASTER_PORT=7077export SPARK_LOCAL_DIRS=/opt/spark/data vim workders1hadoop231 4.添加工作者节点(Master节点) ：编辑workers文件（vim workers），添加工作者节点主机名或IP地址5.配置spark环境参数6.分发配置好的spark安装目录1scp -r spark-3.5.3 hadoop@hadoop231:/opt/spark/ 7.配置spark系统环境变量（SPARK_HOME） (所有节点)1vim /etc/profile 8.进入启动目录12cd /opt/spark/spark-3.5.3/sbinstart-all.sh 9.查看Master节点和Worker节点是否启动12cd /opt/spark/spark-3.5.3/sbinjps 10.Spark程序测试12pysparkspark-shell","tags":[{"name":"Spark","slug":"Spark","permalink":"http://example.com/tags/Spark/"}],"categories":[{"name":"实训","slug":"实训","permalink":"http://example.com/categories/%E5%AE%9E%E8%AE%AD/"}]},{"title":"Spark基础环境配置","date":"2025-06-08T06:25:15.000Z","path":"wiki/Spark基础环境配置/","text":"JDKJDK1.8安装1.双击安装包jdk-8u301-windows-x64.exe 2.安装过程中选择指定的JDK安装目录，比如：D:&#x2F;java&#x2F;jdk1.8.0_301 配置环境变量1.配置JAVA_HOME环境变量 2.配置JDK系统PATH路径 3.配置CLASSPATH环境变量，.:%JAVA_HOME%\\lib&#x2F;dt.jar;%JAVA_HOME%\\lib\\tools.jar; 4.安装成功后的测试，在windows控制台窗口输入如下命令，如果能够看到jdk版本信息说明安装成功 12#安装测试java -version Hadoop添加hadoop用户和组123456# 创建用户组# sudo groupadd hadoop# 创建新的用户sudo adduser hadoop # 会自动创建组名为hadoop的用户组# 将现有用户hadoop添加到用户组hadoop（组名 用户名）# sudo usermod -a -G hadoop hadoop 给新建用户添加管理员权限和sudo权限123# 给新建用户添加root权限和sudo权限sudo usermod -a -G adm hadoopsudo usermod -a -G sudo hadoop 安装并开启SSH服务1sudo apt install openssh-server #安装ssh服务 设置免密登录123456#1、需要在主机上生成公钥和私钥，四次回车ssh-keygen -t rsa#2、将公钥传递给其他虚拟机ssh-copy-id ...... 关闭防火墙，修改安全机制1234567#禁用防火墙systemctl disable firewalld#修改安全机制vi /etc/selinux/config 修改⾥⾯的 SELINUX=disabled 配置hadoop集群配置java，hadoop的环境变量123456#配置主机的环境变量vi /etc/profileexport HADOOP_HOME=/opt/installs/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 123456#拷贝环境变量到另外两台scp -r /etc/profile hadoop@hadoop:/etc/scp -r /etc/profile hadoop@hadoop:/etc/#在01，02，03上分别刷新source /etc/profile 配置分布式集群环境路径：&#x2F;opt&#x2F;installs&#x2F;hadoop&#x2F;etc&#x2F;hadoop 1cd /opt/installs/hadoop/etc/hadoop vi core-site.xml 1234567891011121314&lt;configuration&gt; &lt;!-- 设置namenode节点 --&gt; &lt;!-- 注意: hadoop1.x时代默认端⼝9000 hadoop2.x时代默认端⼝8020 hadoop3.x时 代默认端⼝ 9820 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop:9820&lt;/value&gt; &lt;/property&gt; &lt;!-- hdfs的基础路径，被其他属性所依赖的⼀个基础路径 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/installs/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi hdfs-site.xml 12345678910111213141516&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!--secondarynamenode守护进程的http地址：主机名和端⼝号。参考守护进程布局 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;bigdata02:9868&lt;/value&gt; &lt;/property&gt; &lt;!-- namenode守护进程的http地址：主机名和端⼝号。参考守护进程布局 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;bigdata01:9870&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; vi hadoop-env.sh 1234567export JAVA_HOME=/opt/installs/jdk# Hadoop3中，需要添加如下配置，设置启动集群⻆⾊的⽤户是谁export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root vi workers hadoop和环境变量远程拷贝到其他虚拟机123456#将主机的hadoop 拷贝给集群组员scp -r /opt/installs/hadoop/ hadoop@hadoop:/opt/installs/...#拷贝环境变量scp -r /etc/profile hadoop@hadoop:/etc/... 在02 和 03 上刷新环境变量source &#x2F;etc&#x2F;profile 格式化nameNode1hdfs namenode -format 启动集群1start-all.sh 通过jps查看进程 1$ jps Zookeeper1.执行如下命令安装Kafka：1234$ cd ~/Downloads #假设安装文件放在这个目录下$ sudo tar -zxvf kafka_2.13-3.9.0.tgz -C /opt/kafka$ cd /opt/kafka$ sudo chown -R hadoop:hadoop ./kafka 2.首先需要启动Kafka。请登录Linux系统（本教程统一使用hadoop用户登录），打开一个终端，输入下面命令启动Zookeeper服务：12$ cd /opt/kafka/kafka_2.13-3.9.0$ ./bin/zookeeper-server-start.sh config/zookeeper.properties 注意，执行上面命令以后，终端窗口会返回一堆信息，然后就停住不动了，没有回到Shell命令提示符状态，这时，不要误以为死机了，而是Zookeeper服务器已经启动，正在处于服务状态。所以，不要关闭这个终端窗口，一旦关闭，Zookeeper服务就停止了。 参考资料 大数据平台Hadoop实验环境部署（完全分布式集群模式） Hadoop集群完全分布式搭建","tags":[{"name":"JDK，Hadoop，Zookeeper","slug":"JDK，Hadoop，Zookeeper","permalink":"http://example.com/tags/JDK%EF%BC%8CHadoop%EF%BC%8CZookeeper/"}],"categories":[{"name":"实训","slug":"实训","permalink":"http://example.com/categories/%E5%AE%9E%E8%AE%AD/"}]},{"title":"Hello World","date":"2025-06-04T05:57:11.603Z","path":"wiki/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[],"categories":[]}],"categories":[{"name":"实训","slug":"实训","permalink":"http://example.com/categories/%E5%AE%9E%E8%AE%AD/"}],"tags":[{"name":"Spark HA & Yarn","slug":"Spark-HA-Yarn","permalink":"http://example.com/tags/Spark-HA-Yarn/"},{"name":"Spark","slug":"Spark","permalink":"http://example.com/tags/Spark/"},{"name":"JDK，Hadoop，Zookeeper","slug":"JDK，Hadoop，Zookeeper","permalink":"http://example.com/tags/JDK%EF%BC%8CHadoop%EF%BC%8CZookeeper/"}]}